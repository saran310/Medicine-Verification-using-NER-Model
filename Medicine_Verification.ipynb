{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7k_NgEXuGpx",
        "outputId": "1e6e5fcc-db78-453e-a249-1170b391f8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install requests beautifulsoup4 pandas rapidfuzz pdfplumber camelot-py[cv] tabula-py\n",
        "!apt-get -yqq install ghostscript\n",
        "\n",
        "import os, re, io, json, time, tempfile, warnings\n",
        "import requests\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "from bs4 import BeautifulSoup\n",
        "from rapidfuzz import fuzz, process\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Optional backends\n",
        "import camelot\n",
        "import tabula\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOURCE URLS & CONFIG\n"
      ],
      "metadata": {
        "id": "DMP0beeFuyVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NLEM_URL = \"https://cdsco.gov.in/opencms/resources/UploadCDSCOWeb/2018/UploadConsumer/nlem2022.pdf\"\n",
        "CDSCO_INDEX_URL = \"https://cdsco.gov.in/opencms/opencms/en/Approval_new/Approved-New-Drugs/\"\n",
        "\n",
        "YEAR_RANGE = list(range(2018, 2026))\n",
        "\n",
        "OUT_DIR = \"/content/indian_meds_ref\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "Vl6tNFhxu0m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; ColabParser/1.0)\"}\n",
        "\n",
        "def dl(url, path):\n",
        "    r = requests.get(url, headers=HEADERS, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    with open(path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    return path\n",
        "\n",
        "# Basic text normalization for matching generic names\n",
        "DOSAGE_WORDS = [\n",
        "    r\"\\btablet(s)?\\b\", r\"\\bcapsule(s)?\\b\", r\"\\bsyrup\\b\", r\"\\bsuspension\\b\", r\"\\binjection\\b\",\n",
        "    r\"\\bcream\\b\", r\"\\boxy?ment\\b\", r\"\\bdrops\\b\", r\"\\bsolution\\b\", r\"\\bgel\\b\", r\"\\blotion\\b\",\n",
        "    r\"\\bprefilled pen\\b\", r\"\\bpre-filled pen\\b\", r\"\\bpenfill\\b\", r\"\\bpowder\\b\", r\"\\bpatch\\b\",\n",
        "    r\"\\bdispersible\\b\", r\"\\bextended release\\b\", r\"\\bsustained release\\b\", r\"\\bER\\b\", r\"\\bSR\\b\"\n",
        "]\n",
        "DOSAGE_RE = re.compile(\"|\".join(DOSAGE_WORDS), flags=re.I)\n",
        "STRENGTH_RE = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\s*(mg|mcg|g|ml|iu|%|\\u00b5g)(/\\d+\\s*(ml|g))?\\b\", flags=re.I)\n",
        "PAREN_RE = re.compile(r\"[()\\[\\]{}]\")\n",
        "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "SYNONYM_MAP = {\n",
        "    # common global synonyms\n",
        "    \"acetaminophen\": \"paracetamol\",\n",
        "    \"tylenol\": \"paracetamol\",\n",
        "    \"ibuprofen lysine\": \"ibuprofen\",\n",
        "}\n",
        "\n",
        "\n",
        "def normalize_name(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text.strip().lower()\n",
        "    t = PAREN_RE.sub(\" \", t)\n",
        "    t = STRENGTH_RE.sub(\" \", t)\n",
        "    t = DOSAGE_RE.sub(\" \", t)\n",
        "    t = re.sub(r\"[^a-z0-9 +\\-/]\", \" \", t)\n",
        "    t = MULTISPACE_RE.sub(\" \", t).strip()\n",
        "    # map synonyms\n",
        "    t = SYNONYM_MAP.get(t, t)\n",
        "    t = t.replace(\" & \", \" + \")\n",
        "    return t"
      ],
      "metadata": {
        "id": "I9utJjduvBq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tables_camelot(pdf_path):\n",
        "    tables = []\n",
        "    try:\n",
        "        t_lattice = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')\n",
        "        tables.extend([t.df for t in t_lattice])\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        t_stream = camelot.read_pdf(pdf_path, pages='all', flavor='stream')\n",
        "        tables.extend([t.df for t in t_stream])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return tables\n",
        "\n",
        "\n",
        "def parse_tables_tabula(pdf_path):\n",
        "    try:\n",
        "        dfs = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True, guess=True, lattice=True)\n",
        "        return dfs or []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "def extract_text_pdfplumber(pdf_path):\n",
        "    chunks = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            try:\n",
        "                chunks.append(page.extract_text() or \"\")\n",
        "            except Exception:\n",
        "                chunks.append(\"\")\n",
        "    return \"\\n\".join(chunks)"
      ],
      "metadata": {
        "id": "0uTQIyBhvOu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VERIFY THE DATASET"
      ],
      "metadata": {
        "id": "8xSHggMwv-Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Downloading NLEM 2022…\")\n",
        "nlem_pdf = os.path.join(OUT_DIR, \"nlem2022.pdf\")\n",
        "dl(NLEM_URL, nlem_pdf)\n",
        "\n",
        "print(\"Parsing NLEM tables…\")\n",
        "frames = []\n",
        "for parser in (parse_tables_camelot, parse_tables_tabula):\n",
        "    tbls = parser(nlem_pdf)\n",
        "    for df in tbls:\n",
        "        # Try to find a column that looks like \"Medicine\" or similar\n",
        "        cols = [c.strip().lower() for c in df.columns.astype(str)]\n",
        "        df.columns = cols\n",
        "        for key in [\"medicine\", \"name of medicine\", \"drug\", \"medicine name\", \"generic name\"]:\n",
        "            if key in df.columns:\n",
        "                sub = df[[key]].copy()\n",
        "                sub.columns = [\"medicine_raw\"]\n",
        "                frames.append(sub)\n",
        "                break\n",
        "\n",
        "if not frames:\n",
        "    print(\"Falling back to text extraction for NLEM…\")\n",
        "    text = extract_text_pdfplumber(nlem_pdf)\n",
        "    candidates = []\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if re.search(r\"[A-Za-z]\", line) and not line.lower().startswith((\"section\", \"chapter\")):\n",
        "            candidates.append(line)\n",
        "    nlem_df = pd.DataFrame({\"medicine_raw\": candidates})\n",
        "else:\n",
        "    nlem_df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "nlem_df[\"medicine_clean\"] = nlem_df[\"medicine_raw\"].astype(str).apply(normalize_name)\n",
        "# Filter out empty & obviously bad rows\n",
        "nlem_df = nlem_df[nlem_df[\"medicine_clean\"].str.len() > 1]\n",
        "nlem_df = nlem_df.drop_duplicates(\"medicine_clean\").reset_index(drop=True)\n",
        "nlem_df[\"source\"] = \"NLEM2022\"\n",
        "\n",
        "\n",
        "print(\"Fetching CDSCO Approved New Drugs index…\")\n",
        "html = requests.get(CDSCO_INDEX_URL, headers=HEADERS, timeout=60).text\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "links = []\n",
        "for a in soup.find_all(\"a\", href=True):\n",
        "    href = a[\"href\"].strip()\n",
        "    if href.lower().endswith(\".pdf\") and (\"new\" in href.lower() or \"drug\" in href.lower()):\n",
        "        links.append(href if href.startswith(\"http\") else requests.compat.urljoin(CDSCO_INDEX_URL, href))\n",
        "\n",
        "year_links = []\n",
        "for u in links:\n",
        "    if any(str(y) in u for y in YEAR_RANGE) or \"1971_1980\" in u or \"1991\" in u or \"2019\" in u:\n",
        "        year_links.append(u)\n",
        "\n",
        "year_links = sorted(set(year_links))\n",
        "print(f\"Found {len(year_links)} CDSCO PDFs to parse…\")\n",
        "\n",
        "\n",
        "cdsco_rows = []\n",
        "for i, url in enumerate(year_links, 1):\n",
        "    print(f\"[{i}/{len(year_links)}] {url}\")\n",
        "    pdf_path = os.path.join(OUT_DIR, f\"cdsco_{i}.pdf\")\n",
        "    try:\n",
        "        dl(url, pdf_path)\n",
        "    except Exception as e:\n",
        "        print(\"  download failed:\", e)\n",
        "        continue\n",
        "\n",
        "    found_tables = False\n",
        "    for parser in (parse_tables_camelot, parse_tables_tabula):\n",
        "        tbls = parser(pdf_path)\n",
        "        if not tbls:\n",
        "            continue\n",
        "        for df in tbls:\n",
        "            cols = [c.strip().lower() for c in df.columns.astype(str)]\n",
        "            df.columns = cols\n",
        "            # common header variants\n",
        "            cand_cols = [\n",
        "                \"name of drug\", \"name of new drug\", \"drug name\", \"name of the drug\", \"name\",\n",
        "            ]\n",
        "            for key in cand_cols:\n",
        "                if key in df.columns:\n",
        "                    sub = df[[key]].copy()\n",
        "                    sub.columns = [\"medicine_raw\"]\n",
        "                    sub[\"source_url\"] = url\n",
        "                    cdsco_rows.append(sub)\n",
        "                    found_tables = True\n",
        "                    break\n",
        "        if found_tables:\n",
        "            break\n",
        "\n",
        "    if not found_tables:\n",
        "        text = extract_text_pdfplumber(pdf_path)\n",
        "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "        meds = []\n",
        "        for ln in lines:\n",
        "            if re.match(r\"^\\d+\\.?\\s+\", ln):\n",
        "                # drop serial; take rest up to two columns separated by two+ spaces\n",
        "                rest = re.sub(r\"^\\d+\\.?\\s+\", \"\", ln)\n",
        "                # split on double spaces to approximate columns\n",
        "                part = re.split(r\"\\s{2,}\", rest)[0]\n",
        "                meds.append(part)\n",
        "        if meds:\n",
        "            df_fallback = pd.DataFrame({\"medicine_raw\": meds})\n",
        "            df_fallback[\"source_url\"] = url\n",
        "            cdsco_rows.append(df_fallback)\n",
        "\n",
        "if cdsco_rows:\n",
        "    cdsco_df = pd.concat(cdsco_rows, ignore_index=True)\n",
        "    cdsco_df[\"medicine_clean\"] = cdsco_df[\"medicine_raw\"].astype(str).apply(normalize_name)\n",
        "    cdsco_df = cdsco_df[cdsco_df[\"medicine_clean\"].str.len() > 1]\n",
        "    cdsco_df[\"source\"] = \"CDSCO_NewDrugs\"\n",
        "else:\n",
        "    cdsco_df = pd.DataFrame(columns=[\"medicine_raw\",\"medicine_clean\",\"source\",\"source_url\"])\n",
        "\n",
        "\n",
        "ref_df = pd.concat([\n",
        "    nlem_df[[\"medicine_raw\",\"medicine_clean\",\"source\"]],\n",
        "    cdsco_df[[\"medicine_raw\",\"medicine_clean\",\"source\"] + ([\"source_url\"] if \"source_url\" in cdsco_df.columns else [])]\n",
        "], ignore_index=True).dropna(subset=[\"medicine_clean\"])\\\n",
        " .drop_duplicates(\"medicine_clean\").reset_index(drop=True)\n",
        "\n",
        "ref_path = os.path.join(OUT_DIR, \"indian_medicines_reference.csv\")\n",
        "ref_df.to_csv(ref_path, index=False)\n",
        "print(\"\\nSaved unified reference to:\", ref_path)\n",
        "print(ref_df.head())\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/content/updated_indian_medicine_data.csv\"\n",
        "assert os.path.exists(DATASET_PATH), \"Upload your medicine_data.csv to /content first.\"\n",
        "\n",
        "user_df = pd.read_csv(DATASET_PATH)\n",
        "# Choose what to match on: prefer salt/generic if available, else product name\n",
        "candidate_col = \"salt_composition\" if \"salt_composition\" in user_df.columns else (\"product_name\" if \"product_name\" in user_df.columns else user_df.columns[0])\n",
        "print(\"Matching on column:\", candidate_col)\n",
        "\n",
        "user_df[\"query_raw\"] = user_df[candidate_col].astype(str)\n",
        "user_df[\"query_clean\"] = user_df[\"query_raw\"].apply(normalize_name)\n",
        "\n",
        "choices = ref_df[\"medicine_clean\"].tolist()\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def fuzzy_match_one(s, choices, threshold=90):\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return pd.Series({\"match\": None, \"score\": 0, \"status\": \"No Query\"})\n",
        "    m = process.extractOne(s, choices, scorer=fuzz.ratio)\n",
        "    if m is None:\n",
        "        return pd.Series({\"match\": None, \"score\": 0, \"status\": \"Not Found\"})\n",
        "    name, score, idx = m\n",
        "    status = \"✅ Found\" if score >= threshold else (\"⚠ Close Match\" if score >= 75 else \"❌ Not Found\")\n",
        "    return pd.Series({\"match\": name, \"score\": int(score), \"status\": status})\n",
        "\n",
        "res = user_df[\"query_clean\"].progress_apply(lambda x: fuzzy_match_one(x, choices))\n",
        "report = pd.concat([user_df, res], axis=1)\n",
        "\n",
        "report_path = os.path.join(OUT_DIR, \"verification_report.csv\")\n",
        "report.to_csv(report_path, index=False)\n",
        "print(\"Verification report saved to:\", report_path)\n",
        "\n",
        "# Quick summary\n",
        "summary = report.groupby(\"status\").size().reset_index(name=\"count\").sort_values(\"count\", ascending=False)\n",
        "print(\"\\nSummary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEFZo5woyQ69",
        "outputId": "25bec78f-542e-4d69-86be-d53932e9135d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading NLEM 2022…\n",
            "Parsing NLEM tables…\n",
            "Fetching CDSCO Approved New Drugs index…\n",
            "Found 0 CDSCO PDFs to parse…\n",
            "\n",
            "Saved unified reference to: /content/indian_meds_ref/indian_medicines_reference.csv\n",
            "     medicine_raw  medicine_clean    source source_url\n",
            "0  Glycopyrrolate  glycopyrrolate  NLEM2022        NaN\n",
            "1      Midazolam*       midazolam  NLEM2022        NaN\n",
            "2      Morphine**        morphine  NLEM2022        NaN\n",
            "3             NaN             nan  NLEM2022        NaN\n",
            "4        Medicine        medicine  NLEM2022        NaN\n",
            "Matching on column: salt_composition\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 253973/253973 [00:53<00:00, 4775.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification report saved to: /content/indian_meds_ref/verification_report.csv\n",
            "\n",
            "Summary:\n",
            "           status   count\n",
            "1        ✅ Found  247628\n",
            "2    ❌ Not Found    5616\n",
            "0  ⚠ Close Match     729\n"
          ]
        }
      ]
    }
  ]
}